{
  "text": "This quiz is based on the concepts of image classification using deep learning. It covers Convolutional Neural Networks (CNNs), the role of convolutional and pooling layers, data augmentation, transfer learning, fine-tuning pre-trained models, and visualizing what convnets learn.",
  "quiz": [
    {
      "question": "What is the fundamental type of layer used for image processing in deep learning?",
      "type": "choice",
      "options": [
        "A Dense layer, which processes global patterns in the image.",
        "A Conv2D layer, which processes local patterns.",
        "A MaxPooling2D layer, which learns features.",
        "A Flatten layer, which restructures the image data."
      ],
      "answer": "A Conv2D layer, which processes local patterns.",
      "explanation": "Convolutional layers are the core building block of CNNs, designed to learn and detect local patterns like edges and textures in images."
    },
    {
      "question": "The output of a convolution operation is called a...",
      "type": "blank",
      "answer": "feature map",
      "explanation": "A feature map is a 3D tensor that represents the presence of a specific filter's pattern at different locations in the input."
    },
    {
      "question": "What is the primary purpose of a `MaxPooling2D` layer in a convnet?",
      "type": "choice",
      "options": [
        "To learn complex hierarchical features from the input images.",
        "To apply a series of learnable filters to the input feature map.",
        "To aggressively downsample feature maps.",
        "To increase the resolution of the feature maps for better detail."
      ],
      "answer": "To aggressively downsample feature maps.",
      "explanation": "Max pooling reduces the spatial dimensions of the feature maps, which helps to make the learned features more translation-invariant and reduces the number of parameters."
    },
    {
      "question": "Data augmentation is a technique used primarily to prevent...",
      "type": "blank",
      "answer": "overfitting",
      "explanation": "By creating new, realistic training samples, data augmentation helps the model generalize better to unseen data, thus mitigating overfitting."
    },
    {
      "question": "A typical convnet architecture consists of a stack of `Conv2D` and `MaxPooling2D` layers followed by a...",
      "type": "choice",
      "options": [
        "convolutional base.",
        "classifier.",
        "feature extractor.",
        "data augmenter."
      ],
      "answer": "classifier.",
      "explanation": "The convolutional base extracts features, and the classifier (usually a stack of Dense layers) processes these features to make a final prediction."
    },
    {
      "question": "The number of filters in a `Conv2D` layer corresponds to the ... of the output feature map.",
      "type": "blank",
      "answer": "depth",
      "explanation": "Each filter learns a different pattern, and the output feature map has a depth channel for each filter."
    },
    {
      "question": "What is a key property of the patterns learned by convolutional networks?",
      "type": "choice",
      "options": [
        "They are specific to a single location in the image.",
        "They are translation invariant.",
        "They are always simple color blobs and can never be complex textures.",
        "They are learned by the final dense layers of the network."
      ],
      "answer": "They are translation invariant.",
      "explanation": "Because the same filter is applied across the entire image, a CNN can recognize a pattern regardless of where it appears."
    },
    {
      "question": "The part of a convnet that extracts features is known as the convolutional...",
      "type": "blank",
      "answer": "base",
      "explanation": "The convolutional base is the stack of convolution and pooling layers that transforms the input image into a rich set of feature maps."
    },
    {
      "question": "Which of these is a common data augmentation technique for images?",
      "type": "choice",
      "options": [
        "Random rotation and zoom.",
        "Randomly shuffling the pixels within the image.",
        "Converting the image to a different file format, such as from PNG to JPEG.",
        "Increasing the brightness of every image by a fixed amount."
      ],
      "answer": "Random rotation and zoom.",
      "explanation": "Techniques like random rotation, zoom, shearing, and flipping create believable variations of the training images."
    },
    {
      "question": "The layer used to convert 2D feature maps into a 1D vector is the ... layer.",
      "type": "blank",
      "answer": "Flatten",
      "explanation": "A Flatten layer is required to transition from the convolutional base to the dense classifier."
    },
    {
      "question": "What is transfer learning in the context of computer vision?",
      "type": "choice",
      "options": [
        "Using a pre-trained model on a new, different task.",
        "Training a model from scratch on a new dataset.",
        "Transferring data from one storage device to another for faster access.",
        "Using a model trained for classification on a regression task."
      ],
      "answer": "Using a pre-trained model on a new, different task.",
      "explanation": "Transfer learning leverages the knowledge (features) learned by a model on a large dataset and applies it to a new problem."
    },
    {
      "question": "In transfer learning, the early layers of a convnet learn ... features.",
      "type": "blank",
      "answer": "generic",
      "explanation": "Early layers learn universal features like edges and colors, which are useful for almost any vision task."
    },
    {
      "question": "When using a pre-trained model, the process of keeping the convolutional base's weights unchanged is called...",
      "type": "choice",
      "options": [
        "fine-tuning.",
        "feature extraction.",
        "feature engineering.",
        "model compilation and training."
      ],
      "answer": "feature extraction.",
      "explanation": "In feature extraction, you use the pre-trained model as a fixed feature extractor without updating its weights."
    },
    {
      "question": "To prevent the weights of a layer from being updated during training, you set the layer to be...",
      "type": "blank",
      "answer": "frozen",
      "explanation": "Freezing a layer (e.g., `layer.trainable = False`) ensures that its weights are not modified by the optimizer."
    },
    {
      "question": "The process of unfreezing a few of the top layers of a pre-trained model and training them on a new dataset is called...",
      "type":- "choice",
      "options": [
        "fine-tuning.",
        "feature extraction.",
        "data augmentation.",
        "model initialization from scratch."
      ],
      "answer": "fine-tuning.",
      "explanation": "Fine-tuning adapts the more specialized, higher-level features of the pre-trained model to the new task."
    },
    {
      "question": "The features learned by the deeper layers of a convnet are more ... than those learned by the early layers.",
      "type": "blank",
      "answer": "specialized",
      "explanation": "Deeper layers combine the features from earlier layers to learn more complex and task-specific patterns."
    },
    {
      "question": "A Class Activation Map (CAM) is a technique for...",
      "type": "choice",
      "options": [
        "visualizing which parts of an image a convnet looks at.",
        "augmenting the training data with new samples.",
        "compiling the model with a specific optimizer and loss.",
        "defining the architecture of a new convolutional network."
      ],
      "answer": "visualizing which parts of an image a convnet looks at.",
      "explanation": "CAMs produce heatmaps that highlight the regions of an input image that were important for a particular classification decision."
    },
    {
      "question": "A `Conv2D` layer's 'kernel size' defines the ... of the patch extracted from the input.",
      "type": "blank",
      "answer": "dimensions",
      "explanation": "A kernel size of (3, 3) means the filter will look at 3x3 patches of the input feature map."
    },
    {
      "question": "What is the role of 'padding' in a convolutional layer?",
      "type": "choice",
      "options": [
        "To allow the output feature map to have the same spatial dimensions as the input.",
        "To add random noise to the input image to prevent overfitting.",
        "To reduce the number of parameters in the model.",
        "To increase the depth of the output feature map."
      ],
      "answer": "To allow the output feature map to have the same spatial dimensions as the input.",
      "explanation": "Padding adds rows and columns of zeros around the input, which can counteract the shrinking effect of convolutions."
    },
    {
      "question": "The `strides` parameter in a `Conv2D` layer controls the ... between two successive windows.",
      "type": "blank",
      "answer": "distance",
      "explanation": "A stride of (2, 2) means the filter window moves 2 pixels at a time, which results in downsampling."
    },
    {
      "question": "Why is it generally not a good idea to fine-tune a pre-trained model with a very small learning rate initially?",
      "type": "choice",
      "options": [
        "A small learning rate is recommended for fine-tuning.",
        "A large learning rate is recommended for fine-tuning.",
        "The learning rate does not matter for fine-tuning.",
        "A large learning rate can destroy the pre-trained representations."
      ],
      "answer": "A large learning rate can destroy the pre-trained representations.",
      "explanation": "Fine-tuning should be done with a very small learning rate to avoid making large, disruptive updates to the valuable pre-trained weights."
    },
    {
      "question": "The patterns learned by a single filter in a convnet are visualized as a...",
      "type": "blank",
      "answer": "2D image",
      "explanation": "Visualizing a filter involves creating an image that maximizes the filter's activation, showing what pattern it is tuned to detect."
    },
    {
      "question": "In a well-structured convnet, the depth of feature maps tends to ... while the spatial dimensions tend to ... as you go deeper.",
      "type": "choice",
      "options": [
        "increase, decrease",
        "decrease, increase",
        "increase, increase",
        "decrease, decrease"
      ],
      "answer": "increase, decrease",
      "explanation": "This is a common pattern: pooling layers reduce the height/width, while subsequent convolution layers increase the number of filters (depth)."
    },
    {
      "question": "Transfer learning is most effective when the new task has a ... amount of data.",
      "type": "blank",
      "answer": "small",
      "explanation": "When you have little data, you can't afford to train a model from scratch, so leveraging a pre-trained model is highly beneficial."
    },
    {
      "question": "The final layer of a convnet used for 10-class image classification should have...",
      "type": "choice",
      "options": [
        "10 units and a softmax activation.",
        "1 unit and a sigmoid activation.",
        "10 units and a ReLU activation.",
        "1 unit and a softmax activation."
      ],
      "answer": "10 units and a softmax activation.",
      "explanation": "For multiclass classification, the number of output units must match the number of classes, and softmax is used to produce a probability distribution."
    },
    {
      "question": "Visualizing intermediate activations helps in understanding how an input is ... by the model.",
      "type": "blank",
      "answer": "decomposed",
      "explanation": "These visualizations show how the model breaks down the input into a hierarchy of learned features."
    },
    {
      "question": "What happens to the feature maps as you go deeper into a convnet?",
      "type": "choice",
      "options": [
        "They become more abstract and less visually interpretable.",
        "They become clearer and more visually similar to the original image.",
        "They contain less information about the class of the object.",
        "They retain all the information present in the original input."
      ],
      "answer": "They become more abstract and less visually interpretable.",
      "explanation": "Early layers detect simple patterns like edges, while deeper layers detect more complex, abstract concepts that don't always have a simple visual representation."
    },
    {
      "question": "A pre-trained model like VGG16, trained on ImageNet, is a powerful ... extractor.",
      "type": "blank",
      "answer": "feature",
      "explanation": "Its convolutional base has learned a rich hierarchy of features that are useful for many different computer vision tasks."
    },
    {
      "question": "When should you train a convnet from scratch instead of using transfer learning?",
      "type": "choice",
      "options": [
        "When you have a very large, domain-specific dataset.",
        "When you have a very small dataset.",
        "When your task is a standard problem like cat vs. dog classification.",
        "When you do not have access to powerful GPUs."
      ],
      "answer": "When you have a very large, domain-specific dataset.",
      "explanation": "If you have enough data, you can afford to learn all the features from scratch, which may lead to a better-performing model tailored to your specific domain."
    },
    {
      "question": "The 'convolution' operation in a Conv2D layer is a form of pattern...",
      "type": "blank",
      "answer": "detection",
      "explanation": "The filter (kernel) slides over the input, and the operation produces a high activation where the pattern defined by the filter is detected."
    },
    {
      "question": "When using `ImageDataGenerator` for data augmentation, the transformations are applied...",
      "type": "choice",
      "options": [
        "randomly during the training process.",
        "once to the entire dataset before training begins.",
        "only to the validation and test data.",
        "in a fixed, deterministic way to every image."
      ],
      "answer": "randomly during the training process.",
      "explanation": "The generator applies random transformations to each batch of images on the fly, so the model sees slightly different images at each epoch."
    },
    {
      "question": "The first step in using a pre-trained model is to instantiate its convolutional...",
      "type": "blank",
      "answer": "base",
      "explanation": "You first create an instance of the pre-trained model's base, without its original classifier."
    },
    {
      "question": "Why is a `Flatten` layer necessary before a `Dense` layer in a CNN?",
      "type": "choice",
      "options": [
        "Dense layers expect 1D vector inputs, not 2D feature maps.",
        "Dense layers can only process integer data.",
        "It normalizes the feature values to be between 0 and 1.",
        "It performs a final round of feature extraction."
      ],
      "answer": "Dense layers expect 1D vector inputs, not 2D feature maps.",
      "explanation": "The `Flatten` layer unrolls the multi-dimensional feature map into a single long vector that can be processed by the dense classifier."
    },
    {
      "question": "Pooling layers in a CNN do not have any ... parameters.",
      "type": "blank",
      "answer": "trainable",
      "explanation": "Pooling is a fixed mathematical operation (like taking the max over a window) and does not involve any learned weights."
    },
    {
      "question": "The concept of a hierarchy of features means that...",
      "type": "choice",
      "options": [
        "deeper layers learn more complex patterns based on simpler ones.",
        "all layers in the network learn the exact same features.",
        "the first layer learns the most important features for classification.",
        "the model's features are organized in a tree-like data structure."
      ],
      "answer": "deeper layers learn more complex patterns based on simpler ones.",
      "explanation": "The network learns edges first, then combines them into shapes, then textures, and so on, forming a hierarchy of increasing complexity."
    },
    {
      "question": "When fine-tuning, you should train the new classifier first, before unfreezing the convolutional...",
      "type": "blank",
      "answer": "base",
      "explanation": "You first train the randomly initialized classifier on top of the frozen base. Only after it has stabilized should you unfreeze parts of the base for fine-tuning."
    },
    {
      "question": "What is a major benefit of using convolutions over densely connected layers for images?",
      "type": "choice",
      "options": [
        "They use far fewer parameters.",
        "They use far more parameters.",
        "They are easier to implement in Python from scratch.",
        "They can only be used for grayscale images."
      ],
      "answer": "They use far fewer parameters.",
      "explanation": "By learning local patterns and reusing them across the image (parameter sharing), convnets are much more efficient than dense networks for image data."
    },
    {
      "question": "A `(3, 3)` convolution kernel is a small window that slides over the...",
      "type": "blank",
      "answer": "input",
      "explanation": "The kernel, or filter, is the window that moves across the input image or feature map to detect patterns."
    },
    {
      "question": "If you use a pre-trained model on a new dataset that is very different from the original, you should...",
      "type": "choice",
      "options": [
        "only use the first few layers for feature extraction.",
        "fine-tune the entire model with a large learning rate.",
        "only use the last few layers for feature extraction.",
        "not use transfer learning at all and train from scratch."
      ],
      "answer": "only use the first few layers for feature extraction.",
      "explanation": "If the datasets are very different, only the most generic, low-level features from the early layers are likely to be useful."
    },
    {
      "question": "The input to a `Conv2D` layer is typically a 4D tensor of shape (samples, height, width, ...).",
      "type": "blank",
      "answer": "channels",
      "explanation": "The fourth dimension represents the color channels (e.g., 3 for RGB)."
    },
    {
      "question": "The technique of visualizing filter patterns works by finding an input image that maximizes the filter's...",
      "type": "choice",
      "options": [
        "activation.",
        "size.",
        "color depth.",
        "complexity."
      ],
      "answer": "activation.",
      "explanation": "This is typically done via gradient ascent in input space: modifying an input image to make a specific filter respond as strongly as possible."
    },
    {
      "question": "Max pooling helps achieve a degree of ... invariance.",
      "type": "blank",
      "answer": "translation",
      "explanation": "By taking the maximum value in a local patch, the network becomes less sensitive to the exact position of a feature within that patch."
    },
    {
      "question": "When should you use feature extraction instead of fine-tuning?",
      "type": "choice",
      "options": [
        "When the new dataset is very small and similar to the original.",
        "When the new dataset is very large and different from the original.",
        "When you have access to unlimited computational resources.",
        "When the pre-trained model is very shallow."
      ],
      "answer": "When the new dataset is very small and similar to the original.",
      "explanation": "If the dataset is small, you risk overfitting if you try to fine-tune. If it's similar, the pre-trained features are likely sufficient without modification."
    },
    {
      "question": "The classifier part of a CNN is also known as its...",
      "type": "blank",
      "answer": "head",
      "explanation": "The classifier is often called the 'head' of the network, which sits on top of the convolutional 'base'."
    },
    {
      "question": "The primary reason transfer learning is so successful in computer vision is that...",
      "type": "choice",
      "options": [
        "many visual features are generic and reusable.",
        "all image datasets have the exact same statistical properties.",
        "pre-trained models are computationally cheaper to run for inference.",
        "modern GPUs are specifically designed for transfer learning."
      ],
      "answer": "many visual features are generic and reusable.",
      "explanation": "The hierarchy of features learned on a general dataset like ImageNet (edges, textures, parts of objects) is a useful starting point for most other vision tasks."
    },
    {
      "question": "Data augmentation happens only during...",
      "type": "blank",
      "answer": "training",
      "explanation": "Augmentation should not be applied to the validation or test data, as you want to evaluate the model on the original, unmodified images."
    },
    {
      "question": "What is the typical shape of the input for a classifier in a CNN?",
      "type": "choice",
      "options": [
        "A 1D vector of features.",
        "A 3D tensor representing the image.",
        "A 2D matrix of pixel values.",
        "A single scalar value representing the image."
      ],
      "answer": "A 1D vector of features.",
      "explanation": "The output of the convolutional base is flattened into a 1D vector before being passed to the dense classifier."
    },
    {
      "question": "The process of reducing the spatial size of feature maps is called...",
      "type": "blank",
      "answer": "downsampling",
      "explanation": "Max pooling is a common way to achieve downsampling in convolutional neural networks."
    },
    {
      "question": "To get a class activation heatmap, you compute a weighted sum of the...",
      "type": "choice",
      "options": [
        "feature maps from the final convolutional layer.",
        "weights of the final dense layer.",
        "input image pixels.",
        "activations from the first convolutional layer."
      ],
      "answer": "feature maps from the final convolutional layer.",
      "explanation": "The heatmap is formed by weighting the feature maps of the last conv layer by the gradient of the class with respect to those maps."
    }
  ]
}
